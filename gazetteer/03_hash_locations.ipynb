{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T16:24:11.906774Z",
     "start_time": "2020-05-01T16:24:11.630281Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sqlite3\n",
    "import string\n",
    "import zipfile\n",
    "from hashlib import sha256\n",
    "from pathlib import Path\n",
    "from pprint import pp\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import regex\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T16:24:12.751363Z",
     "start_time": "2020-05-01T16:24:12.749031Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path('..') / 'data'\n",
    "INTERIM_DIR = DATA_DIR / '01_interim'\n",
    "\n",
    "IN_DB = INTERIM_DIR / 'gazetteer_02_idigbio_2020-03-30.db'\n",
    "OUT_DB = INTERIM_DIR / 'gazetteer_03_idigbio_2020-03-30.db'\n",
    "\n",
    "CHUNK = 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T16:57:53.662195Z",
     "start_time": "2020-05-01T16:57:53.655443Z"
    }
   },
   "outputs": [],
   "source": [
    "if OUT_DB.exists():\n",
    "    os.remove(OUT_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T16:29:03.081062Z",
     "start_time": "2020-05-01T16:29:03.078663Z"
    }
   },
   "outputs": [],
   "source": [
    "def dict_factory(cursor, row):\n",
    "    dict_ = {}\n",
    "    for idx, col in enumerate(cursor.description):\n",
    "        dict_[col[0]] = row[idx]\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T19:11:10.147250Z",
     "start_time": "2020-04-30T19:11:10.142012Z"
    }
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "    drop index if exists occ_l_hash;\n",
    "    drop index if exists occ_v_hash;\n",
    "    drop index if exists raw_l_hash;\n",
    "    drop index if exists raw_v_hash;\n",
    "\n",
    "    create index if not exists occ_coreid on occurrence (coreid);\n",
    "    create index if not exists raw_coreid on occurrence_raw (coreid);\n",
    "    \"\"\"\n",
    "\n",
    "with sqlite3.connect(IN_DB) as cxn:\n",
    "    cxn.executescript(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build Hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Get the columns that contribute to the hashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T16:24:38.223100Z",
     "start_time": "2020-05-01T16:24:38.219173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_columns(table, db=IN_DB):\n",
    "    specials = \"\"\"\n",
    "        coreid l_hash v_hash\n",
    "        locality verbatimLocality \"\"\".split()\n",
    "\n",
    "    sql = f'PRAGMA table_info({table});'\n",
    "\n",
    "    with sqlite3.connect(db) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "        columns = [r[1] for r in cxn.execute(sql) if r[1] not in specials]\n",
    "\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The queries are used to build the hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T19:11:10.161298Z",
     "start_time": "2020-04-30T19:11:10.157112Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_sql(table):\n",
    "    select = f\"\"\"select * from {table};\"\"\"\n",
    "    update = ' '.join(f\"\"\"\n",
    "        update {table}\n",
    "           set l_hash = ?, v_hash = ?\n",
    "         where coreid = ?;\n",
    "        \"\"\".split())\n",
    "    return select, update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Build the hashes and write them to the database.\n",
    "\n",
    "Python hashes use a different hash seed for every run so we are using the another hashing function where we can control the hash seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T19:11:10.167881Z",
     "start_time": "2020-04-30T19:11:10.162796Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def write_hashes(select, update, l_cols, v_cols):\n",
    "    batch = []\n",
    "\n",
    "    with sqlite3.connect(IN_DB) as cxn:\n",
    "        cxn.row_factory = dict_factory\n",
    "\n",
    "        for row in tqdm(cxn.execute(select)):\n",
    "\n",
    "            l_hash, v_hash = None, None\n",
    "\n",
    "            if row['locality']:\n",
    "                l_hash = b'|'.join(str(row[c]).encode() for c in l_cols)\n",
    "                l_hash = sha256(l_hash).hexdigest()\n",
    "\n",
    "            if (row['verbatimLocality']\n",
    "                    and row['locality'] != row['verbatimLocality']):\n",
    "                v_hash = b'|'.join(str(row[c]).encode() for c in v_cols)\n",
    "                v_hash = sha256(v_hash).hexdigest()\n",
    "\n",
    "            batch.append([l_hash, v_hash, row['coreid']])\n",
    "\n",
    "            if len(batch) >= CHUNK:\n",
    "                cxn.executemany(update, batch)\n",
    "                cxn.commit()\n",
    "                batch = []\n",
    "\n",
    "        if batch:\n",
    "            cxn.executemany(update, batch)\n",
    "            cxn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Wrap the entire hash building process in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T19:11:10.174244Z",
     "start_time": "2020-04-30T19:11:10.169136Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_hashes(table):\n",
    "    columns = get_columns(table)\n",
    "\n",
    "    l_cols = ['locality'] + columns\n",
    "    v_cols = ['verbatimLocality'] + columns\n",
    "\n",
    "    select, update = build_sql(table)\n",
    "\n",
    "    write_hashes(select, update, l_cols, v_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hashes for occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T19:32:38.930395Z",
     "start_time": "2020-04-30T19:11:10.175510Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60666995it [21:22, 47306.29it/s] \n"
     ]
    }
   ],
   "source": [
    "build_hashes('occurrence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hashes for occurrence_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T20:01:47.662528Z",
     "start_time": "2020-04-30T19:32:38.931553Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60708188it [29:00, 34876.02it/s]\n"
     ]
    }
   ],
   "source": [
    "build_hashes('occurrence_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create Indexes on Hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T20:06:24.702897Z",
     "start_time": "2020-04-30T20:01:47.663916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "    create index occ_l_hash on occurrence (l_hash);\n",
    "    create index occ_v_hash on occurrence (v_hash);\n",
    "    create index raw_l_hash on occurrence_raw (l_hash);\n",
    "    create index raw_v_hash on occurrence_raw (v_hash);\n",
    "    \"\"\"\n",
    "\n",
    "with sqlite3.connect(IN_DB) as cxn:\n",
    "    cxn.executescript(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull One Representative from Every Group of Hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T17:02:23.082273Z",
     "start_time": "2020-05-01T17:02:23.079998Z"
    }
   },
   "outputs": [],
   "source": [
    "hashes = \"\"\"\n",
    "    with hashes as (\n",
    "            select l_hash as hash \n",
    "             from occurrence_raw\n",
    "            where hash is not null\n",
    "        union all\n",
    "            select v_hash as hash\n",
    "              from occurrence_raw\n",
    "             where hash is not null\n",
    "    )\n",
    "    select distinct hash\n",
    "      from hashes;\n",
    "\"\"\"\n",
    "\n",
    "represent = \"\"\"\n",
    "    select *\n",
    "      from occurrence_raw\n",
    "     where l_hash = ?\n",
    "        or v_hash = ?\n",
    "     limit 1;\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T17:02:32.227869Z",
     "start_time": "2020-05-01T17:02:32.225499Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = get_columns('occurrence_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T17:02:41.488332Z",
     "start_time": "2020-05-01T17:02:41.462863Z"
    }
   },
   "outputs": [],
   "source": [
    "create = f\"\"\"\n",
    "    create table gazetteer (hash,source,locality,{','.join(columns)})\n",
    "\"\"\"\n",
    "with sqlite3.connect(OUT_DB) as cxn_out:\n",
    "    cxn_out.execute(create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T17:02:51.212828Z",
     "start_time": "2020-05-01T17:02:51.209709Z"
    }
   },
   "outputs": [],
   "source": [
    "insert = f\"\"\"\n",
    "    insert into gazetteer (hash,source,locality,{','.join(columns)})\n",
    "    values ({','.join(['?'] * (len(columns) + 3))})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T17:14:42.447025Z",
     "start_time": "2020-05-01T17:03:00.976096Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16270050it [11:38, 23282.92it/s]\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "\n",
    "with sqlite3.connect(IN_DB) as cxn_in:\n",
    "    cxn_in.row_factory = dict_factory\n",
    "\n",
    "    with sqlite3.connect(OUT_DB) as cxn_out:\n",
    "\n",
    "        for hash_row in tqdm(cxn_in.execute(hashes)):\n",
    "            hash_ = hash_row['hash']\n",
    "\n",
    "            for row in cxn_in.execute(represent, (hash_, hash_)):\n",
    "\n",
    "                if row['l_hash'] == hash_:\n",
    "                    loc = 'locality'\n",
    "                else:\n",
    "                    loc = 'verbatimLocality'\n",
    "\n",
    "                fields = [row[c] if row[c] != '' else None for c in columns]\n",
    "\n",
    "                batch.append([hash_, loc, row[loc]] + fields)\n",
    "\n",
    "                if len(batch) >= CHUNK:\n",
    "                    cxn_out.executemany(insert, batch)\n",
    "                    cxn_out.commit()\n",
    "                    batch = []\n",
    "\n",
    "        if batch:\n",
    "            cxn_out.executemany(insert, batch)\n",
    "            cxn_out.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
